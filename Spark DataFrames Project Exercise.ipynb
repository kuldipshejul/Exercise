{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa5f29d1-fb92-48fc-be5d-7e23ff8f80b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Spark DataFrames Project Exercise "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d29beb99-e864-4273-87c1-23f4d26e5afa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let's get some quick practice with your new Spark DataFrame skills, you will be asked some basic questions about some stock market data, in this case Walmart Stock from the years 2012-2017. This exercise will just ask a bunch of questions, unlike the future machine learning exercises, which will be a little looser and be in the form of \"Consulting Projects\", but more on that later!\n",
    "\n",
    "For now, just answer the questions and complete the tasks below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9cf3a89-4390-4988-817f-610bab5e8e6b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Use the walmart_stock.csv file to Answer and complete the  tasks below!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0b5b6a6-65ad-42fe-88dc-fb8a952082b1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Start a simple Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9497037-eb33-445a-ab08-b6479e215943",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version\n",
    "sys.version_info\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "724afdbc-e0cb-4abf-9013-a8f8b8f2a765",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import findspark\n",
    "#findspark.init(\"/home/stuthi/prep/spark-2.0.0-bin-hadoop2.7\")\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('Walmart Project').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a35d73e9-3056-44e6-a14f-78e68d019e22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Load the Walmart Stock CSV File, have Spark infer the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d14d3e8d-037c-4792-8d80-10f109d86608",
     "showTitle": false,
     "title": ""
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = spark.read.csv('walmart_stock.csv',inferSchema=True,header = True)\n",
    "access_key = 'AKIA2UC3CHGOHTVVOBPU'\n",
    "secret_key = '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'\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", access_key)\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", secret_key)\n",
    "\n",
    "# If you are using Auto Loader file notification mode to load files, provide the AWS Region ID.\n",
    "aws_region = \"ap-south-1\"\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.\" + aws_region + \".amazonaws.com\")\n",
    "\n",
    "df = spark.read.option(\"delimiter\", \",\").csv('s3://sample-bucket-3499/walmart_stock.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76204a1b-af54-450a-a967-b1a203b15477",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### What are the column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4bb76b03-6df0-425f-9b86-a8a480f1b20a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84a32d9a-e7cd-4ed7-a8be-8e9a3a4dd90a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### What does the Schema look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07b7455a-61a2-4f2a-bcf3-87c63032fe9f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b236f50f-8c5a-4a1c-93a5-5a0310ea5fd1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Print out the first 5 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b12115a-c911-4ec6-bbdb-1e5d80ca9f05",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8a6c9ee-600d-4eae-a8de-d027e94b72f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Use describe() to learn about the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e9eab78-ec37-49a2-ae86-b3842c992012",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f68c839-052c-402d-b718-43a9588d20cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Bonus Question!\n",
    "#### There are too many decimal places for mean and stddev in the describe() dataframe. Format the numbers to just show up to two decimal places. Pay careful attention to the datatypes that .describe() returns, we didn't cover how to do this exact formatting, but we covered something very similar. [Check this link for a hint](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.Column.cast)\n",
    "\n",
    "If you get stuck on this, don't worry, just view the solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a03154c8-9d5b-4a8d-84bd-2760a45ec6e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.describe().printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd551d05-fc33-4450-a20a-ade85842d1d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import format_number\n",
    "result = df.describe()\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42ee1538-4f0b-4860-a799-c2a443682659",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result.select(result['summary'],\n",
    "             format_number(result['Open'].cast('float'),2).alias('Open'),\n",
    "             format_number(result['High'].cast('float'),2).alias('High')).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "587d4dc9-0553-4b74-a03b-cf8667f97914",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0bb8ddc2-9810-4d77-96b2-411a38e36d8b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### Create a new dataframe with a column called HV Ratio that is the ratio of the High Price versus volume of stock traded for a day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3b2d9e3-485a-4b18-b774-b2d0a2378ad6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.withColumn('HV Ratio',df['High']/df['Volume']).select('HV Ratio').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8d38643-6f0a-4fc2-9585-6a62a7f2da49",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### What day had the Peak High in Price?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e69c4d4d-3be7-489c-880f-de23c6e623e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.orderBy(df['High'].desc()).head(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f1d4967-cad2-47f9-af12-3bcb32102451",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### What is the mean of the Close column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c0e9cd8-6f0b-4ff8-a8d4-a2b79ab6eba7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df.select(mean(df['Close'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6351451a-7bcf-44ff-89db-afb4e83656fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### What is the max and min of the Volume column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1c85206-61e1-414b-a8ad-ea8fe6068c50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min,max\n",
    "df.select(min(df['Volume']),max(df['Volume'])).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad863d8a-2ad2-43b1-be79-d6ff9767c025",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49069bc3-5b27-4bbd-9f66-159c8d32a47d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### How many days was the Close lower than 60 dollars?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccb9573c-c6a8-483b-a9ca-db4e36420c2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df[df['Close']<60].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d27ddf-6446-42f2-a869-045853bd3be5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.filter(df['Close']<60).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49e22ae8-44b2-48f4-bd95-5be6436fb2fc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### What percentage of the time was the High greater than 80 dollars ?\n",
    "#### In other words, (Number of Days High>80)/(Total Days in the dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "339fc52c-86e9-4caf-98b3-4fe4bf147c2a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(((df.filter(\"High > 80\")).count())/ df.count() )*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a660f251-e620-447c-b9c4-eecf26b33017",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### What is the Pearson correlation between High and Volume?\n",
    "#### [Hint](http://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameStatFunctions.corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7ce3ae4-324f-446a-93a8-30f75598a10c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr\n",
    "df.select(corr(df['High'],df['Volume'])).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3273c357-3577-4cce-9664-172feaabf955",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### What is the max High per year?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f99428c5-5b6e-4924-96be-87df92972399",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "ydf = df.withColumn(\"Year\",year(df['Date']))\n",
    "maxdf = ydf.groupby(\"Year\").max()\n",
    "maxdf.select(['max(High)',\"Year\"]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e0804d3-1da9-4d6c-8b97-d5b98cc1362f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### What is the average Close for each Calendar Month?\n",
    "#### In other words, across all the years, what is the average Close price for Jan,Feb, Mar, etc... Your result will have a value for each of these months. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4abb7d94-96b7-4843-80c3-59e3631edb5a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month\n",
    "mdf = df.withColumn(\"Month\",month(df['Date']))\n",
    "amdf = mdf.groupBy('Month').mean()\n",
    "# amdf.head(2)\n",
    "a = amdf.select(['Month','avg(Close)']).orderBy('Month',asc=True)\n",
    "# a.describe()\n",
    "a.select([format_number(a['avg(Close)'].cast(\"float\"),3).alias(\"Avg Close\"), \"Month\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5861667-a512-4bcf-b52d-13ef51fadcaa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "312206e8-446e-45e4-a6b4-632bc388fdd5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# FOR EACH 5 EXAMPLE \n",
    "# 1. Select\n",
    "# 2. select + filter \n",
    "# 3. select + filter + count\n",
    "# 4. select + withColumn \n",
    "# 5. select + withColumnRenamed\n",
    "# 6. withColmn \n",
    "# 7. withColumnRenamed \n",
    "# 8. groupBy + orderBy + having \n",
    "# 9. select + groupBy\n",
    "# 10. groupBy + aggregation sum\n",
    "# 10. groupBy + aggregation max\n",
    "# 10. groupBy + aggregation min \n",
    "# 10. groupBy + aggregation avg\n",
    "# 11. Join \n",
    "# 11. Join + select\n",
    "# 11. Join + groupBy\n",
    "# 12 window + count\n",
    "# 12 window + sum\n",
    "# 13 window + max\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df2 = df.filter(F.col('High') > 55)\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f553c0e-9457-40f9-867d-82bf6b9eb514",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d3faeb1-1865-42dc-ae18-79bb26e824c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#with out column import lib\n",
    "df.select(df.Date,df.High).show()\n",
    "df.select(df.Date,df.High,df.Volume).show()\n",
    "df.select(\"*\").show()\n",
    "\n",
    "df.select(\"Date\",\"Low\",\"Volume\").show()\n",
    "df.select(\"Date\",\"High\",\"Volume\").show()\n",
    "df.select(\"Date\",\"Adj Close\",\"Volume\").show()\n",
    "df.select(\"Date\",\"Close\",\"Volume\").show()\n",
    "df.select(\"Date\",\"Open\",\"Volume\").show()\n",
    "\n",
    "df.select(df.Date,df.Low,df.Open,df.High,df.Volume).show()\n",
    "df.select(df.Date,df.Close).show()\n",
    "\n",
    "#with column lib \n",
    "\n",
    "df.select(F.col(\"Date\")).show()\n",
    "df.select(\"*\").show()\n",
    "df.select(F.col(\"Date\"),F.col(\"Close\"),F.col(\"Volume\")).show()\n",
    "df.select(\"*\").show()\n",
    "df.select(F.col(\"Date\"),F.col(\"Volume\")).show()\n",
    "#df.select([x for x in df.columns]).show()\n",
    "\n",
    "\n",
    "#df.select([2:4]).show(5)   # Here in show in parathesis row of dataframe\n",
    "#df.select([:4]).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "870d11a7-57c0-4a8b-8149-035e2414c2f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#  2. select + fil\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71137b56-977f-4b25-be44-1339da88bc80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. select + fil#\n",
    "#dataframe.select('column_name').where(dataframe.column condition)\n",
    "\n",
    "#df.select(df.Date,df.High, df.Volume).filter(df.High == 60).show()\n",
    "#df.select(F(\"Date\"),F(\"High\"),F(\"low\")).filter(F(\"High\") > 60).show()\n",
    "\n",
    "#df.select(df.High,df.Date,df.Volume,df.Low).filter(df.Low != 60).show()\n",
    "#df.select(df.Date,df.Low).filter(df.Date==\"2012-01-31\").show()\n",
    "\n",
    "df.select(df.Date, df.High, df.Low, df.Volume).filter((df.High > 60) & (df.Close > 40)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08081aea-9aeb-4d36-aa77-2e964f8fff98",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ec76ee2-a1c4-4a36-aa17-3ee5a905aa82",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#select + filter + count\n",
    "\n",
    "df.select(df.count(\"*\")).filter((df.High) > 60).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d3a17a0-382f-4ea0-9689-618a146029a3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# select + withColumn\n",
    "from pyspark.sql.functions import col,lit\n",
    "#df.withColumnRenamed(\"High\",\"HIGH\").show()\n",
    "#df.withColumn(\"HIGH\",col(\"HIGH\")*100).show()\n",
    "df.withColumn(\"All_time_High\",F.lit(\"100\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "448785f7-9012-4369-a4f2-b5569026cc8c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8. groupBy + orderBy + Filter \n",
    "#from pyspark.sql.functions import month, sum\n",
    "\n",
    "#df.groupBy(month(df.Date)).sum(df.Open).show()\n",
    "#df.groupBy(month(df.Date)).sum(df.Close).show()\n",
    "\n",
    "\n",
    "#df.groupBy(month(df.Date)).sort(df.Volume.asc()).show()\n",
    "#df.groupBy(month(df.Date)).filter(df.Volume > 200).show()\n",
    "#df.groupBy(month(df.Date)).sort(df.open.desc()).filter(df.close > 200).show()\n",
    "#df.groupBy(month(df.Date)).sort(df.close.desc()).filter(df.Close > 150).show()\n",
    "\n",
    "#how to find month wise high \n",
    "\n",
    "df.select(\"*\").groupBy(month(df.Date)).count().filter(F.col(\"count\") > 100).orderBy(\"count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92682a60-5694-4ef5-b75e-2d6626708f80",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#select + groupBy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c447c1d-db62-47e8-b343-4ca677611556",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 10. groupBy + aggregation sum"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark DataFrames Project Exercise",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
